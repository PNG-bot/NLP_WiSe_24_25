{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKIcqKhuj6wklpe0qTjndD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PNG-bot/NLP_WiSe_24_25/blob/MUFF_NLP_24_25/eigenes_projekt/eigenes_projekt_zeitschriften_einlesen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eigenes Projekt - Zeitschriften aus DZA einlesen"
      ],
      "metadata": {
        "id": "F4OJMDKqnd4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aus API einlesen (DZA)\n"
      ],
      "metadata": {
        "id": "CrGvsdta7yAT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dS5wYW0nncCv"
      },
      "outputs": [],
      "source": [
        "# Python-Bibliothek pysolr installieren\n",
        "%pip install -q pysolr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Suchindizes der Deutsche Digitale Bibliothek"
      ],
      "metadata": {
        "id": "-QZUTZEd4hHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Deutsche Digitale Bibliothek betreibt Solr-Suchindizes, die für die verschiedenen Funktionen der (Sub-) Portale benötigt werden. Das Zeitungsportal benutzt zwei Suchindizes. Eine weiterführende Dokumentation befindet sich hier: https://api.deutsche-digitale-bibliothek.de/#/search/getSolrSearch\n",
        "\n",
        "newspaper: enthält Informationen über Zeitungstitel\n",
        "Schema: https://dev.fiz-karlsruhe.de/stash/projects/DDB/repos/ddb-backend/browse/Cortex/conf/solr/newspaper/conf/schema.xml\n",
        "Konfiguration: https://dev.fiz-karlsruhe.de/stash/projects/DDB/repos/ddb-backend/browse/Cortex/conf/solr/newspaper/conf/solrconfig.xml\n",
        "\n",
        "newspaper-issues: enthält die zeitungsbezogenen Metadaten inkl. Volltexte\n",
        "Schema: https://dev.fiz-karlsruhe.de/stash/projects/DDB/repos/ddb-backend/browse/Cortex/conf/solr/newspaper-issues/conf/schema.xml\n",
        "Konfiguration: https://dev.fiz-karlsruhe.de/stash/projects/DDB/repos/ddb-backend/browse/Cortex/conf/solr/newspaper-issues/conf/solrconfig.xml"
      ],
      "metadata": {
        "id": "92gPZ5sN7wSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per Suchindex newspapaer abfragen\n"
      ],
      "metadata": {
        "id": "ANjORE1Y3eif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pysolr\n",
        "\n",
        "# Solr-Endpunkt-URL\n",
        "solr_url = 'https://api.deutsche-digitale-bibliothek.de/2/search/index/newspaper'\n",
        "\n",
        "# Solr-Client initialisieren\n",
        "solr = pysolr.Solr(solr_url, timeout=10)\n",
        "\n",
        "# Suchparameter\n",
        "q = {\n",
        "    'q': 'location:\"buenos aires\" AND hasLoadedIssues:true',\n",
        "    'fl': 'id,title,location,frequency,progress',\n",
        "    'rows': 10  # Anzahl der zurückzugebenden Ergebnisse\n",
        "}\n",
        "\n",
        "# Suche ausführen\n",
        "results = solr.search(**q)\n",
        "\n",
        "# Ergebnisse ausgeben\n",
        "for result in results:\n",
        "    print(f\"ID: {result.get('id', 'N/A')}\")\n",
        "    print(f\"Title: {result.get('title', 'N/A')}\")\n",
        "    print(f\"Location: {result.get('location', 'N/A')}\")\n",
        "    print(f\"Frequency: {result.get('frequency', 'N/A')}\")\n",
        "    print(f\"Progress: {result.get('progress', 'N/A')}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "Qshs2O6l3nsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Per Suchindex newspaper-issue abfragen\n"
      ],
      "metadata": {
        "id": "aG8X6PIu337D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python-Bibliothek pandas installieren\n",
        "%pip install -q pandas\n",
        "%pip install -q pysolr"
      ],
      "metadata": {
        "id": "P-NIAewF36gM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pysolr\n",
        "\n",
        "solr_url = 'https://api.deutsche-digitale-bibliothek.de/2/search/index/newspaper-issues'\n",
        "solr = pysolr.Solr(solr_url, always_commit=True, timeout=10)\n",
        "\n",
        "q = {\n",
        "    'q': 'zdb_id:2149754-0 AND type:issue',\n",
        "    'rows': 1000\n",
        "}\n",
        "\n",
        "response = solr.search(**q)\n",
        "\n",
        "# Überführen der Ergebnisse in ein Pandas DataFrame\n",
        "df = pd.DataFrame(response.docs)\n",
        "\n",
        "# DataFrame anzeigen\n",
        "df"
      ],
      "metadata": {
        "id": "x26NqaTm4LVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datenanalyse\n",
        "#### Puplikationszeitraum ermitteln\n"
      ],
      "metadata": {
        "id": "mBEfMrcX49dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sicherstellen, dass publication_date als Datumswerte formatiert sind\n",
        "df['publication_date'] = pd.to_datetime(df['publication_date'])\n",
        "\n",
        "# Frühestes und spätestes Datum ermitteln\n",
        "earliest_date = df['publication_date'].min()\n",
        "latest_date = df['publication_date'].max()\n",
        "\n",
        "# Ergebnisse anzeigen\n",
        "print(f\"Frühestes Veröffentlichungsdatum: {earliest_date}\")\n",
        "print(f\"Spätestes Veröffentlichungsdatum: {latest_date}\")"
      ],
      "metadata": {
        "id": "gyyWeWim5Jg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### METS / MODS Dateien herunterladen und daraus Volltesxt Seiten generieren\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sUrkSXNX6AGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python-Bibliothek requests installieren\n",
        "%pip install -q requests\n"
      ],
      "metadata": {
        "id": "ll9rUcXa6b5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "\n",
        "# Verzeichnis für die XML-Dateien erstellen\n",
        "directory = 'La_Otra_Alemania'\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Durch jede Zeile des DataFrames iterieren\n",
        "for index, row in df.iterrows():\n",
        "    # Extrahiere den Wert der Spalte id\n",
        "    item_id = row['id']\n",
        "\n",
        "    # Formatiere den Wert der Spalte publication_date im Format YYYY-MM-DD\n",
        "    publication_date = row['publication_date'].strftime('%Y-%m-%d')\n",
        "\n",
        "    # Generiere die URL zur API-Abfrage\n",
        "    url = f'https://api.deutsche-digitale-bibliothek.de/2/items/{item_id}/source/record'\n",
        "\n",
        "    # Setze die HTTP-Header\n",
        "    headers = {\n",
        "        'Accept': 'application/xml'\n",
        "    }\n",
        "\n",
        "    # API-Anfrage senden\n",
        "    response = requests.get(url, headers=headers)\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        # Erstelle einen Dateipfad für die XML-Datei\n",
        "        file_path = os.path.join(directory, f'{publication_date}_{item_id}.xml')\n",
        "\n",
        "        # Speichere die XML-Datei im erstellten Verzeichnis\n",
        "        with open(file_path, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        print(f'Datei gespeichert: {file_path}')\n",
        "    else:\n",
        "        print(f'Fehler beim Abrufen der Datei für ID {item_id}: {response.status_code}')\n",
        "\n",
        "print('Fertig!')"
      ],
      "metadata": {
        "id": "6ickt9-b6epe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python-Bibliothek lxml installieren\n",
        "%pip install -q lxml"
      ],
      "metadata": {
        "id": "OAcMBRkp6JXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rom lxml import etree\n",
        "\n",
        "# Definiere die Namensräume\n",
        "NAMESPACES = {\n",
        "    'mets': 'http://www.loc.gov/METS/',\n",
        "    'xlink': 'http://www.w3.org/1999/xlink'\n",
        "}\n",
        "\n",
        "# Verzeichnisse definieren\n",
        "xml_directory = 'La_Otra_Alemania'\n",
        "download_directory = 'La_Otra_Alemania'\n",
        "\n",
        "# Erstelle das Download-Verzeichnis, falls es nicht existiert\n",
        "if not os.path.exists(download_directory):\n",
        "    os.makedirs(download_directory)\n",
        "\n",
        "# Funktion, um URLs aus einer XML-Datei zu extrahieren und herunterzuladen\n",
        "def download_files_from_xml(xml_file_path, xpath_expr, subfolder, extension):\n",
        "    # Lade die XML-Datei ein\n",
        "    with open(xml_file_path, 'rb') as xml_file:\n",
        "        tree = etree.parse(xml_file)\n",
        "\n",
        "    # Extrahiere die URLs mit dem gegebenen XPath-Ausdruck\n",
        "    urls = tree.xpath(xpath_expr, namespaces=NAMESPACES)\n",
        "\n",
        "    # Erstelle das Unterverzeichnis, benannt nach der XML-Datei\n",
        "    xml_file_name = os.path.splitext(os.path.basename(xml_file_path))[0]\n",
        "    destination_dir = os.path.join(download_directory, xml_file_name, subfolder)\n",
        "    os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "    # Lade jede URL herunter und speichere die Datei\n",
        "    for i, url in enumerate(urls, start=1):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # Überprüfe auf HTTP-Fehler\n",
        "            file_path = os.path.join(destination_dir, f'{i}.{extension}')\n",
        "            with open(file_path, 'wb') as output_file:\n",
        "                output_file.write(response.content)\n",
        "            print(f'Downloaded {url} to {file_path}')\n",
        "        except requests.RequestException as e:\n",
        "            print(f'Failed to download {url}: {e}')\n",
        "\n",
        "# Durchlaufe alle XML-Dateien im Verzeichnis\n",
        "for xml_file in os.listdir(xml_directory):\n",
        "    if xml_file.endswith('.xml'):\n",
        "        xml_file_path = os.path.join(xml_directory, xml_file)\n",
        "        # Lade und speichere Dateien mit dem ersten XPath-Ausdruck\n",
        "        download_files_from_xml(xml_file_path, '//mets:mets/mets:fileSec/mets:fileGrp[@USE=\"DEFAULT\"]/mets:file/mets:FLocat/@xlink:href', 'DEFAULT', 'jpeg')\n",
        "        # Lade und speichere Dateien mit dem zweiten XPath-Ausdruck\n",
        "        download_files_from_xml(xml_file_path, '//mets:mets/mets:fileSec/mets:fileGrp[@USE=\"DDB_FULLTEXT\"]/mets:file/mets:FLocat/@xlink:href', 'DDB_FULLTEXT', 'xml')"
      ],
      "metadata": {
        "id": "V5n767Ib6KZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Alto-XML-Sateien einlesen"
      ],
      "metadata": {
        "id": "Xn66Q_ZY790r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vorbereitung\n"
      ],
      "metadata": {
        "id": "vEIuKC9-8Cud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e038d51d-89f1-45e2-b769-52b4f794931f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from lxml import etree\n",
        "from tqdm import tqdm\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Lokale Installation\n"
      ],
      "metadata": {
        "id": "kPXC9AC38M_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q tqdm\n",
        "%pip install -q bs4\n",
        "%pip install -q pathlib\n",
        "%pip install -q matplotlib\n",
        "%pip install -q numpy\n",
        "%pip install -q wordcloud\n",
        "%pip install -q plotly.express\n",
        "%pip install -q spacy\n",
        "%pip install -q nltk"
      ],
      "metadata": {
        "id": "i5FZACq4-hMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exemplarisches Laden in EINER XML-Datei"
      ],
      "metadata": {
        "id": "Hhc47ovz-uqH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Als einfache Datei laden"
      ],
      "metadata": {
        "id": "OrGMW1pn_L-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"La_Otra_Alemania/1942-01-01_BWSVNEKFTM7SPW4SQAWQQNGIHHTRFDS4/DDB_FULLTEXT/5.xml\") as f:\n",
        "    content = f.read()\n",
        "print(content)\n",
        "print(type(content))"
      ],
      "metadata": {
        "id": "r0OnH5nl_E1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Direkt als XML laden"
      ],
      "metadata": {
        "id": "fAB9U8vI_OhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xml = BeautifulSoup(open('La_Otra_Alemania/1942-01-01_BWSVNEKFTM7SPW4SQAWQQNGIHHTRFDS4/DDB_FULLTEXT/5.xml'),'lxml-xml')\n",
        "print(xml.prettify())\n",
        "print(type(xml))"
      ],
      "metadata": {
        "id": "97KWXlIj_R5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "c5YnQP_P-Vm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laden aller Dateien aus einem Unterordner"
      ],
      "metadata": {
        "id": "Z1kmq5cD_eVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folder_xml = []\n",
        "\n",
        "for filepath in Path('./La_Otra_Alemania/1942-01-01_BWSVNEKFTM7SPW4SQAWQQNGIHHTRFDS4').glob('*/*.XML'):\n",
        "    with filepath.open() as f:\n",
        "        soup = BeautifulSoup(f,'lxml-xml')\n",
        "        folder_xml.append(soup)\n",
        "print(len(folder_xml))\n",
        "print(folder_xml)"
      ],
      "metadata": {
        "id": "VIxm9SmK_ik0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laden ALLER .xml-Dateien im Unterverzeichnis"
      ],
      "metadata": {
        "id": "jo46xkin_wY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_xml = {}\n",
        "\n",
        "for filepath in Path('./La_Otra_Alemania').glob('**/*.XML'):\n",
        "    with filepath.open(encoding='utf-8') as f:\n",
        "        # read as string:\n",
        "        xml_string = f.read()\n",
        "        #Dateiname inklusive der Namen der beiden übergeordneten Ordner als Schlüssel:\n",
        "        key = f\"{filepath.parent.parent.name}/{filepath.parent.name}/{filepath.name}\"\n",
        "        all_xml[key] = xml_string\n",
        "print(len(all_xml))"
      ],
      "metadata": {
        "id": "U3aqbGaN_2Ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Kurze Erkärung zu .glob:"
      ],
      "metadata": {
        "id": "E53SaPBnAAFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Mustererkennung: Die glob-Methode wird verwendet, um Dateipfade mit einem bestimmten Muster abzugleichen. In diesem Fall ist das Muster '\\*/\\*.XML'.\n",
        "- Musterdetails:\n",
        "    - '\\*' entspricht einer beliebigen Anzahl von Zeichen, einschließlich keinem.\n",
        "    - '\\*.XML' entspricht jeder Datei mit der Erweiterung '.XML'.\n",
        "    - Das Muster '\\*/\\*.XML' sucht speziell nach '.XML'-Dateien, die sich eine Verzeichnisebene unterhalb des angegebenen Pfads ('./Folder') befinden.\n",
        "- Rekursive Suche: Das Muster '\\*/\\*.XML' sucht nach XML-Dateien in allen unmittelbaren Unterverzeichnissen von './Folder'. Um rekursiv durch alle Unterverzeichnisse in beliebiger Tiefe zu suchen, wird das Muster '\\*\\*/\\*.XML' verwendet."
      ],
      "metadata": {
        "id": "01jVm6yXACWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Umwandlung in ein Pandas-Dataframe"
      ],
      "metadata": {
        "id": "pJLfVyEgAF1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(list(all_xml.items()), columns=['filename', 'content'])\n",
        "df"
      ],
      "metadata": {
        "id": "eAFdrIvtAUKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Suchen nach möglichen Problemen (optional)"
      ],
      "metadata": {
        "id": "Raj2MzzIAYiq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sucht nach dem Index von Reihen, in denen die Spalte \"text\" keinen Inhalt hat:\n",
        "empty_text_indices = df[df['text'] == \"\"].index\n",
        "\n",
        "# Ausgabe der gefundenen Zeilen ohne Text:\n",
        "for index in empty_text_indices:\n",
        "    print(f\"Index: {index}, Content: {df.loc[index, 'Filename']}\")"
      ],
      "metadata": {
        "id": "Pf1TmQweAbeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text aus XML extrahieren\n"
      ],
      "metadata": {
        "id": "WQxfrhrrAfZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(content):\n",
        "    # Remove XML declaration if present\n",
        "    if content.startswith('<?xml'):\n",
        "        content = content.split('?>', 1)[1]\n",
        "\n",
        "    NS = {'alto': 'http://www.loc.gov/standards/alto/ns-v2#'}\n",
        "    tree = etree.fromstring(unicodedata.normalize(\"NFC\", content))\n",
        "\n",
        "    text_lines = []  # Initialize as an empty list to store text lines\n",
        "\n",
        "    for line in tree.xpath('//alto:TextLine', namespaces=NS):\n",
        "        text = \" \".join(\n",
        "            word for word in line.xpath('alto:String/@CONTENT', namespaces=NS))\n",
        "        text_lines.append(text)  # Append each extracted text line to the list\n",
        "\n",
        "    return \" \".join(text_lines)  # Return all text as a single string"
      ],
      "metadata": {
        "id": "GieDi54pAiMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Erstellen einer neuen Spalte \"text\". In diese wird das jeweilige Ergebnis der Anwendung der Funktion \"extract_text\" auf das korrespondierende Element in der Spalte \"content\" des Dataframes \"df\" geschrieben:"
      ],
      "metadata": {
        "id": "bHD6aWbdAzFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['content'].apply(extract_text)\n",
        "df"
      ],
      "metadata": {
        "id": "gnSvjINOA2Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Löschen der Spalte \"content\" um Speicherplatz zu sparen:"
      ],
      "metadata": {
        "id": "CF6mX72GA3d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.drop(columns=['content'])\n",
        "df2"
      ],
      "metadata": {
        "id": "tJBeprpWA-Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speichern als CSV:\n"
      ],
      "metadata": {
        "id": "0nirixeEBByk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.to_csv(\"otra_alemania_content.csv\", encoding = \"UTF-8\")"
      ],
      "metadata": {
        "id": "xAeGNKVqBFaw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}